# Set the model name and server base URL here
model: "Llama-3.2-1B"        # served model name that you set in vllm server
base_url: "http://localhost:8000"  # or http://{ip_address}:{port}
tokenizer: "path_tokenizer"  # path that includes tokenizer file in which you set in vllm server
# for dataset random, you need to set tokenizer and it will generate random dataset based on it

# input_tokens and output_tokens are the number of tokens in the input and output text, respectively.
# for example, input_tokens: 256, output_tokens: 256 --> [256, 256]
input_output:
  - [256, 256]
  - [2048, 2048]
# max_concurrency is the maximum number of concurrent requests that can be sent to the server.
# num_prompts is the number of prompts to be sent to the server.
# for example, max_concurrency: 1, num_prompts: 10 --> [1, 10]
concurrency_prompts:
  - [1, 10]
  - [4, 40]
  - [8, 80]
  - [16, 160]
  - [32, 320]